import os
import torch
from transformers import pipeline
import time
import traceback
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
import uvicorn
import nest_asyncio
from pyngrok import ngrok

# --- è¨­å®š ---
# ãƒ¢ãƒ‡ãƒ«åã‚’è¨­å®š
MODEL_NAME = "google/gemma-2-2b-jpn-it"  # ãŠå¥½ã¿ã®ãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›´å¯èƒ½ã§ã™
print(f"ãƒ¢ãƒ‡ãƒ«åã‚’è¨­å®š: {MODEL_NAME}")

TEXT_SUMMRIZE_MODEL = "facebook/bart-large-cnn"  # ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®è¨­å®š
print(f"è¦ç´„ãƒ¢ãƒ‡ãƒ«åã‚’è¨­å®š: {TEXT_SUMMRIZE_MODEL}")

# --- FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å®šç¾© ---
app = FastAPI(
    title="ãƒ­ãƒ¼ã‚«ãƒ«LLM APIã‚µãƒ¼ãƒ“ã‚¹",
    description="transformersãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®ãŸã‚ã®API",
    version="1.0.0"
)

# CORSãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢ã‚’è¿½åŠ 
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«å®šç¾© ---
class Message(BaseModel):
    role: str
    content: str

# ç›´æ¥ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ãŸç°¡ç•¥åŒ–ã•ã‚ŒãŸãƒªã‚¯ã‚¨ã‚¹ãƒˆ
class SimpleGenerationRequest(BaseModel):
    prompt: str
    max_new_tokens: Optional[int] = 512
    do_sample: Optional[bool] = True
    temperature: Optional[float] = 0.7
    top_p: Optional[float] = 0.9

class SummarizationRequest(BaseModel):
    text: str
    do_sample: Optional[bool] = False
    temperature: Optional[float] = 0.7
    top_p: Optional[float] = 0.9

class GenerationResponse(BaseModel):
    generated_text: str
    response_time: float

# --- ãƒ¢ãƒ‡ãƒ«é–¢é€£ã®é–¢æ•° ---
# ãƒ¢ãƒ‡ãƒ«ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
class Model_Manager:
    def __init__(self):
        self.chat_model = None
        self.text_summarize_model = None
    
    def load_model(self, type, model_name):
        """ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        try:
            if type == "chat":
                if self.chat_model.model.config._name_or_path == model_name:
                    print("ãƒ¢ãƒ‡ãƒ«ã¯ã™ã§ã«èª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã™ã€‚")
                    return
                print("chatãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã‚’é–‹å§‹...")
                self.chat_model = pipeline(
                    "text-generation",
                    model=model_name,
                    device=0 if torch.cuda.is_available() else -1,
                    max_length=512,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9
                )
                print("ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            if type == "summarization":
                if self.text_summarize_model.model.config._name_or_path == model_name:
                    print("è¦ç´„ãƒ¢ãƒ‡ãƒ«ã¯ã™ã§ã«èª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã™ã€‚")
                    return
                print("è¦ç´„ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã‚’é–‹å§‹...")
                self.text_summarize_model = pipeline(
                    "summarization",
                    model=model_name,
                    device=0 if torch.cuda.is_available() else -1,
                    max_length=300,
                    do_sample=True
                )
                print("è¦ç´„ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            traceback.print_exc()
            self.chat_model = None
            self.text_summarize_model = None
global model
model = Model_Manager()

def extract_assistant_response(outputs, user_prompt):
    """ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‹ã‚‰ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”ã‚’æŠ½å‡ºã™ã‚‹"""
    assistant_response = ""
    try:
        if outputs and isinstance(outputs, list) and len(outputs) > 0 and outputs[0].get("generated_text"):
            generated_output = outputs[0]["generated_text"]
            
            if isinstance(generated_output, list):
                # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®å ´åˆ
                if len(generated_output) > 0:
                    last_message = generated_output[-1]
                    if isinstance(last_message, dict) and last_message.get("role") == "assistant":
                        assistant_response = last_message.get("content", "").strip()
                    else:
                        # äºˆæœŸã—ãªã„ãƒªã‚¹ãƒˆå½¢å¼ã®å ´åˆã¯æœ€å¾Œã®è¦ç´ ã‚’æ–‡å­—åˆ—ã¨ã—ã¦è©¦è¡Œ
                        print(f"è­¦å‘Š: æœ€å¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å½¢å¼ãŒäºˆæœŸã—ãªã„ãƒªã‚¹ãƒˆå½¢å¼ã§ã™: {last_message}")
                        assistant_response = str(last_message).strip()

            elif isinstance(generated_output, str):
                # æ–‡å­—åˆ—å½¢å¼ã®å ´åˆ
                full_text = generated_output
                
                # å˜ç´”ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥åŠ›ã®å ´åˆã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¾Œã®å…¨ã¦ã‚’æŠ½å‡º
                if user_prompt:
                    prompt_end_index = full_text.find(user_prompt)
                    if prompt_end_index != -1:
                        prompt_end_pos = prompt_end_index + len(user_prompt)
                        assistant_response = full_text[prompt_end_pos:].strip()
                    else:
                        # å…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ãã®ã¾ã¾è¿”ã™
                        assistant_response = full_text
                else:
                    assistant_response = full_text
            else:
                print(f"è­¦å‘Š: äºˆæœŸã—ãªã„å‡ºåŠ›ã‚¿ã‚¤ãƒ—: {type(generated_output)}")
                assistant_response = str(generated_output).strip()  # æ–‡å­—åˆ—ã«å¤‰æ›

    except Exception as e:
        print(f"å¿œç­”ã®æŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        traceback.print_exc()
        assistant_response = "å¿œç­”ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸã€‚"  # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨­å®š

    if not assistant_response:
        print("è­¦å‘Š: ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚å®Œå…¨ãªå‡ºåŠ›:", outputs)
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¾ãŸã¯ã‚¨ãƒ©ãƒ¼å¿œç­”ã‚’è¿”ã™
        assistant_response = "å¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

    return assistant_response

# --- FastAPIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå®šç¾© ---
@app.on_event("startup")
async def startup_event():
    """èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–"""
    model.load_model("chat", MODEL_NAME)  # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã¯ãªãåŒæœŸçš„ã«èª­ã¿è¾¼ã‚€
    model.load_model("summarization", TEXT_SUMMRIZE_MODEL)
    if model.chat_model is None or model.text_summarize_model is None:
        print("è­¦å‘Š: èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
    else:
        print("èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

@app.get("/")
async def root():
    """åŸºæœ¬çš„ãªAPIãƒã‚§ãƒƒã‚¯ç”¨ã®ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {"status": "ok", "message": "Local LLM API is runnning"}

@app.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    models = []
    models.append({"status": "error", "message": "ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"} if model.chat_model is None else {"status": "ok", "message": "ãƒ¢ãƒ‡ãƒ«ãŒæ­£å¸¸ã«èª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã™ã€‚"})
    models.append({"status": "error", "message": "è¦ç´„ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"} if model.text_summarize_model is None else {"status": "ok", "message": "è¦ç´„ãƒ¢ãƒ‡ãƒ«ãŒæ­£å¸¸ã«èª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã™ã€‚"})
    return models

# ç°¡ç•¥åŒ–ã•ã‚ŒãŸã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.post("/generate", response_model=GenerationResponse)
async def generate_simple(request: SimpleGenerationRequest):
    """å˜ç´”ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥åŠ›ã«åŸºã¥ã„ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""

    if model.chat_model is None:
        print("generateã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚èª­ã¿è¾¼ã¿ã‚’è©¦ã¿ã¾ã™...")
        raise HTTPException(status_code=503, detail="ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚å¾Œã§ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")

    try:
        start_time = time.time()
        print(f"ã‚·ãƒ³ãƒ—ãƒ«ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å—ä¿¡: prompt={request.prompt[:100]}..., max_new_tokens={request.max_new_tokens}")  # é•·ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯åˆ‡ã‚Šæ¨ã¦

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ã‚­ã‚¹ãƒˆã§ç›´æ¥å¿œç­”ã‚’ç”Ÿæˆ
        print("ãƒ¢ãƒ‡ãƒ«æ¨è«–ã‚’é–‹å§‹...")
        outputs = model(
            request.prompt,
            max_new_tokens=request.max_new_tokens,
            do_sample=request.do_sample,
            temperature=request.temperature,
            top_p=request.top_p,
        )
        print("ãƒ¢ãƒ‡ãƒ«æ¨è«–ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

        end_time = time.time()
        response_time = end_time - start_time
        print(f"å¿œç­”ç”Ÿæˆæ™‚é–“: {response_time:.2f}ç§’")

        return GenerationResponse(
            generated_text=outputs[0].summary_text,
            response_time=response_time
        )

    except Exception as e:
        print(f"ã‚·ãƒ³ãƒ—ãƒ«å¿œç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"å¿œç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
    
@app.post("/summarize", response_model=GenerationResponse)
async def summarize_text(request:SummarizationRequest):
    summarization_model = model.text_summarize_model
    if summarization_model is None:
        print("summarizeã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: è¦ç´„ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        raise HTTPException(status_code=503, detail="è¦ç´„ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚å¾Œã§ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
    try:
        start_time = time.time()
        print(f"è¦ç´„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å—ä¿¡: text={request.text[:100]}..., do_sample={request.do_sample}")  # é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã¯åˆ‡ã‚Šæ¨ã¦

        # è¦ç´„ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦è¦ç´„ã‚’ç”Ÿæˆ
        print("è¦ç´„ãƒ¢ãƒ‡ãƒ«æ¨è«–ã‚’é–‹å§‹...")
        outputs = summarization_model(
            request.text,
            do_sample=request.do_sample,
            temperature=request.temperature,
            top_p=request.top_p,
        )
        print("è¦ç´„ãƒ¢ãƒ‡ãƒ«æ¨è«–ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå¿œç­”ã‚’æŠ½å‡º
        assistant_response = extract_assistant_response(outputs, request.text)
        print(f"æŠ½å‡ºã•ã‚ŒãŸã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå¿œç­”: {assistant_response[:100]}...")  # é•·ã„å ´åˆã¯åˆ‡ã‚Šæ¨ã¦

        end_time = time.time()
        response_time = end_time - start_time
        print(f"å¿œç­”ç”Ÿæˆæ™‚é–“: {response_time:.2f}ç§’")

        return GenerationResponse(
            generated_text=assistant_response,
            response_time=response_time
        )
    except Exception as e:
        print(f"è¦ç´„å¿œç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"è¦ç´„ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

print("FastAPIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’å®šç¾©ã—ã¾ã—ãŸã€‚")

# --- ngrokã§APIã‚µãƒ¼ãƒãƒ¼ã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•° ---
def run_with_ngrok(port=8501):
    """ngrokã§FastAPIã‚¢ãƒ—ãƒªã‚’å®Ÿè¡Œ"""
    nest_asyncio.apply()

    ngrok_token = os.environ.get("NGROK_TOKEN")
    if not ngrok_token:
        print("Ngrokèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ãŒ'NGROK_TOKEN'ç’°å¢ƒå¤‰æ•°ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        try:
            print("Colab Secrets(å·¦å´ã®éµã‚¢ã‚¤ã‚³ãƒ³)ã§'NGROK_TOKEN'ã‚’è¨­å®šã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚")
            ngrok_token = input("Ngrokèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ (https://dashboard.ngrok.com/get-started/your-authtoken): ")
        except EOFError:
            print("\nã‚¨ãƒ©ãƒ¼: å¯¾è©±å‹å…¥åŠ›ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
            print("Colab Secretsã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚»ãƒ«ã§`os.environ['NGROK_TOKEN'] = 'ã‚ãªãŸã®ãƒˆãƒ¼ã‚¯ãƒ³'`ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®šã—ã¦ãã ã•ã„")
            return

    if not ngrok_token:
        print("ã‚¨ãƒ©ãƒ¼: Ngrokèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ä¸­æ­¢ã—ã¾ã™ã€‚")
        return

    try:
        ngrok.set_auth_token(ngrok_token)

        # æ—¢å­˜ã®ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‰ã˜ã‚‹
        try:
            tunnels = ngrok.get_tunnels()
            if tunnels:
                print(f"{len(tunnels)}å€‹ã®æ—¢å­˜ãƒˆãƒ³ãƒãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚é–‰ã˜ã¦ã„ã¾ã™...")
                for tunnel in tunnels:
                    print(f"  - åˆ‡æ–­ä¸­: {tunnel.public_url}")
                    ngrok.disconnect(tunnel.public_url)
                print("ã™ã¹ã¦ã®æ—¢å­˜ngrokãƒˆãƒ³ãƒãƒ«ã‚’åˆ‡æ–­ã—ã¾ã—ãŸã€‚")
            else:
                print("ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªngrokãƒˆãƒ³ãƒãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        except Exception as e:
            print(f"ãƒˆãƒ³ãƒãƒ«åˆ‡æ–­ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            # ã‚¨ãƒ©ãƒ¼ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšç¶šè¡Œã‚’è©¦ã¿ã‚‹

        # æ–°ã—ã„ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‹ã
        print(f"ãƒãƒ¼ãƒˆ{port}ã«æ–°ã—ã„ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‹ã„ã¦ã„ã¾ã™...")
        ngrok_tunnel = ngrok.connect(port)
        public_url = ngrok_tunnel.public_url
        print("---------------------------------------------------------------------")
        print(f"âœ… å…¬é–‹URL:   {public_url}")
        print(f"ğŸ“– APIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ (Swagger UI): {public_url}/docs")
        print("---------------------------------------------------------------------")
        print("(APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚„ãƒ–ãƒ©ã‚¦ã‚¶ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ã«ã“ã®URLã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ãã ã•ã„)")
        uvicorn.run(app, host="0.0.0.0", port=port, log_level="info")  # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’infoã«è¨­å®š

    except Exception as e:
        print(f"\n ngrokã¾ãŸã¯Uvicornã®èµ·å‹•ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        traceback.print_exc()
        # ã‚¨ãƒ©ãƒ¼å¾Œã«æ®‹ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‰ã˜ã‚ˆã†ã¨ã™ã‚‹
        try:
            print("ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šæ®‹ã£ã¦ã„ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‰ã˜ã¦ã„ã¾ã™...")
            tunnels = ngrok.get_tunnels()
            for tunnel in tunnels:
                ngrok.disconnect(tunnel.public_url)
            print("ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‰ã˜ã¾ã—ãŸã€‚")
        except Exception as ne:
            print(f"ngrokãƒˆãƒ³ãƒãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã«åˆ¥ã®ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {ne}")

# --- ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯ ---
if __name__ == "__main__":
    # æŒ‡å®šã•ã‚ŒãŸãƒãƒ¼ãƒˆã§ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•
    run_with_ngrok(port=8501)  # ã“ã®ãƒãƒ¼ãƒˆç•ªå·ã‚’ç¢ºèª
    # run_with_ngrokãŒçµ‚äº†ã—ãŸã¨ãã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
    print("\nã‚µãƒ¼ãƒãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ãŒçµ‚äº†ã—ã¾ã—ãŸã€‚")